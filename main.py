# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bP0yMveJDnhneO43FYa2bMyXKPsrEB9l
"""
import argparse
from environment import FrozenLake, play
from model_based import policy_iteration, value_iteration
from tabular_model_free_algorithms import sarsa, q_learning
from non_tabular_model_free_algorithms import linear_sarsa, linear_q_learning, LinearWrapper

def parse_arguments():
    """
        Parse the command line arguments of the program.
    """

    parser = argparse.ArgumentParser(description='Exploring reinfocement lerning algorithms')
    parser.add_argument(
        "-l",
        "--lake",
        type=int,
        nargs="?",
        help="Choose lake type \n \tSmall Lake(1) \n \tBig Lake(2)",
        default = 1
    )
    parser.add_argument(
        "-m",
        "--model",
        type=int,
        nargs="?",
        help="Choose model to evaluate on Lake \n \tModel: Based (1) \n \tTabular model-free (2) \n\tNon-Tabular model-free (3) \n\tAll (4) \n\t Human Interface(5)",
        default = 4
    )
    parser.add_argument(
         "-g",
         "--gamma",
         type=float,
         nargs="?",
         help="white value for discont factor",
         default = 0.9
     )
    parser.add_argument(
         "-t",
         "--theta",
         type=float,
         nargs="?",
         help="initialize the value for theta",
         default = 0.001
     )
    parser.add_argument(
         "-i",
         "--max_iterations",
         type=int,
         nargs="?",
         help="maximum number of iterations for policy and value iterations",
         default = 100
     )
    parser.add_argument(
         "-e",
         "--max_episodes",
         type=int,
         nargs="?",
         help="maximum number of episodes to train",
         default = 2000
     )
    parser.add_argument(
         "-r",
         "--eta",
         type=float,
         nargs="?",
         help="initializing learning rate",
         default = 0.5
     )
    parser.add_argument(
         "-ep",
         "--epsilon",
         type=float,
         nargs="?",
         help="initializing epsilon value",
         default = 0.5
     )
    return parser.parse_args()
def main():
    seed = 0
    args = parse_arguments()
    
    max_steps = 0
    # Small lake
    if args.lake == 1:
        lake =   [['&', '.', '.', '.'],
                  ['.', '#', '.', '#'],
                  ['.', '.', '.', '#'],
                  ['#', '.', '.', '$']]
        max_steps = 16
    elif args.lake == 2: 
        lake = [['&', '.', '.', '.', '.', '.', '.', '.' ],
                ['.', '.', '.', '.', '.', '.', '.', '.' ],
                ['.', '.', '.', '#', '.', '.', '.', '.' ],
                ['.', '.', '.', '.', '.', '#', '.', '.' ],
                ['.', '.', '.', '#', '.', '.', '.', '.' ],
                ['.', '#', '#', '.', '.', '.', '#', '.' ],
                ['.', '#', '.', '.', '.', '.', '#', '.' ],
                ['.', '.', '.', '.', '.', '.', '.', '$' ]]
        max_steps = 65
    else:
        raise Exception('invalid input, check -h --help to know optional input argument deatails')

    env = FrozenLake(lake, slip=0.1, max_steps=max_steps, seed=seed)
    gamma = args.gamma
    theta = args.theta
    max_iterations = args.max_iterations
    max_episodes = args.max_episodes
    eta = args.eta
    epsilon = args.epsilon
    
    if args.model == 1 or args.model == 4:
        print('# Model-based algorithms')
        
        print('')
        
        print('## Policy iteration')
        policy, value = policy_iteration(env, gamma, theta, max_iterations)
        env.render(policy, value)
        
        print('')
        
        print('## Value iteration')
        policy, value = value_iteration(env, gamma, theta, max_iterations)
        env.render(policy, value)
        
        print('')
        
    if args.model == 2 or args.model == 4:
        print('# Model-free algorithms')
        
        print('')
        
        print('## Sarsa')
        policy, value = sarsa(env, max_episodes, eta, gamma, epsilon, seed=seed)
        env.render(policy, value)
        
        print('')
        
        print('## Q-learning')
        policy, value = q_learning(env, max_episodes, eta, gamma, epsilon, seed=seed)
        env.render(policy, value)
        
        print('')
        
    if args.model == 3 or args.model == 4:
        linear_env = LinearWrapper(env)
        
        print('## Linear Sarsa')
        
        parameters = linear_sarsa(linear_env, max_episodes, eta,
                                  gamma, epsilon, seed=seed)
        policy, value = linear_env.decode_policy(parameters)
        linear_env.render(policy, value)
        
        print('')
        
        print('## Linear Q-learning')
        
        parameters = linear_q_learning(linear_env, max_episodes, eta,
                                       gamma, epsilon, seed=seed)
        policy, value = linear_env.decode_policy(parameters)
        linear_env.render(policy, value)
    if args.model == 5:
        print("'W' to move up ‚¨ÜÔ∏è \n'A' to move Left ‚¨ÖÔ∏è\n'S' to move Down ‚¨áÔ∏è\n'D' to move Right ‚û°Ô∏è\n'@' is agent ü§ñ\n'#' represents hole üí© \n'$' is Destination üòò")
        play(env)
main()