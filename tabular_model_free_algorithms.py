# -*- coding: utf-8 -*-
"""Tabular_Model_free_Algorithms.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14k_H7P_VVOIdN0tDl60tb46YGTANMLNz
"""
import numpy as np
"""
#Sarsa
"""

def sarsa(env, max_episodes, eta, gamma, epsilon, seed=None):
    random_state = np.random.RandomState(seed)
    random_state
    eta = np.linspace(eta, 0, max_episodes)
    epsilon = np.linspace(epsilon, 0, max_episodes)
    
    q = np.zeros((env.n_states, env.n_actions))
    
    for i in range(max_episodes):
        s = env.reset()
        #TODO
        if np.random.rand() < epsilon[i] or max(q[s]) == 0:
            a= np.random.choice(env.n_actions)
        else:
            a = np.argmax(q[s])
        terminal = False
        while not terminal:
            next_s, r, terminal = env.step(a)
            if np.random.rand() < epsilon[i] or max(q[next_s]) == 0:
                next_a= np.random.choice(env.n_actions)
            else:
                next_a = np.argmax(q[next_s])
            q[s, a] = q[s, a] + eta[i]*(r + gamma*q[next_s, next_a] - q[s, a])
            s = next_s
            a = next_a
  
    policy = q.argmax(axis=1)
    value = q.max(axis=1)
        
    return policy, value

"""#Q-learning"""

def q_learning(env, max_episodes, eta, gamma, epsilon, seed=None):
    random_state = np.random.RandomState(seed)
    random_state
    eta = np.linspace(eta, 0, max_episodes)
    epsilon = np.linspace(epsilon, 0, max_episodes)
    
    q = np.zeros((env.n_states, env.n_actions))
    
    for i in range(max_episodes):
        s = env.reset()
        #TODO
        terminal = False
        while not terminal:
            if np.random.random() > (1 -epsilon[i]) or max(q[s]) == 0:
                a= np.random.randint(env.n_actions)
            else:
                a = np.argmax(q[s])
            next_s, r, terminal = env.step(a)
            q[s, a] = max([q[s, a] + eta[i]*(r + gamma*q[next_s, next_a] - q[s, a]) for next_a in range(env.n_actions)])
            s = next_s   
    policy = q.argmax(axis=1)
    value = q.max(axis=1)
        
    return policy, value